# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rXyTxal1rea4HJwF7DyGv907tyQI1U4H
"""

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd

data=pd.read_csv('stocks_data.csv')

# data = pd.read_csv('/content/drive/MyDrive/Project/stocks_data-2.csv')

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
import warnings
import pickle
import os
import numpy as np

# Suppress warnings
warnings.filterwarnings("ignore")

# Load the dataset


# Ensure the 'Date' column is in datetime format
data['Date'] = pd.to_datetime(data['Date'])

# Create directories to save models
model_dirs = ['/content/drive/MyDrive/haha/models/arima',
              '/content/drive/MyDrive/haha/models/lstm',
              '/content/drive/MyDrive/haha/models/randomforest',
              '/content/drive/MyDrive/haha/models/xgboost']
for dir in model_dirs:
    os.makedirs(dir, exist_ok=True)

# Group data by category
grouped_data = data.groupby('category')

for category, group in grouped_data:
    # Define features and target variable
    X = group[['Open', 'High', 'Low']]
    y = group['Close']

    # Use only a subset of the data to speed up the training process
    train_size = 1000
    X_train = X.iloc[-train_size:]  # Last 1000 records for training
    y_train = y.iloc[-train_size:]  # Last 1000 records for training

    # Train ARIMA model
    arima_model = ARIMA(y_train, order=(5, 1, 0), exog=X_train)
    arima_model_fit = arima_model.fit()
    with open(f'/content/drive/MyDrive/haha/models/arima/model_{category.replace("/", "_")}.pkl', 'wb') as f:
        pickle.dump(arima_model_fit, f)

    # Train LSTM model
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))

    lstm_model = Sequential()
    lstm_model.add(LSTM(50, activation='relu', input_shape=(1, X_train_scaled.shape[1])))
    lstm_model.add(Dense(1))
    lstm_model.compile(optimizer='adam', loss='mse')
    lstm_model.fit(X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1])),
                   y_train_scaled, epochs=50, verbose=0)

    with open(f'/content/drive/MyDrive/haha/models/lstm/model_{category.replace("/", "_")}.pkl', 'wb') as f:
        pickle.dump({'model': lstm_model, 'scaler_X': scaler_X, 'scaler_y': scaler_y}, f)

    # Train Random Forest model
    rf_model = RandomForestRegressor(n_estimators=100)
    rf_model.fit(X_train, y_train)
    with open(f'/content/drive/MyDrive/haha/models/randomforest/model_{category.replace("/", "_")}.pkl', 'wb') as f:
        pickle.dump({'model': rf_model}, f)

    # Train XGBoost model
    xgboost_model = xgb.XGBRegressor()
    xgboost_model.fit(X_train, y_train)
    with open(f'/content/drive/MyDrive/haha/models/xgboost/model_{category.replace("/", "_")}.pkl', 'wb') as f:
        pickle.dump({'model': xgboost_model}, f)

# from flask import Flask, request, jsonify
# import pandas as pd
# import numpy as np
# import pickle
# import xgboost as xgb
# from tensorflow.keras.models import load_model
# import logging

# # Set up logging
# logging.basicConfig(level=logging.DEBUG)

# app = Flask(__name__)

# # Load the dataset to get category options
# try:
#     # data = pd.read_csv('/content/drive/MyDrive/haha/stocks_data.csv')
#     categories = data['category'].unique().tolist()
# except Exception as e:
#     logging.error(f"Failed to load categories: {e}")
#     categories = []

# # Load the pre-trained models
# model_paths = {
#     'ARIMA': '/content/drive/MyDrive/haha/models/arima',
#     'LSTM': '/content/drive/MyDrive/haha/models/lstm',
#     'Random Forest': '/content/drive/MyDrive/haha/models/randomforest',
#     'XGBoost': '/content/drive/MyDrive/haha/models/xgboost'
# }

# models = {model_type: {} for model_type in model_paths.keys()}

# for model_type, path in model_paths.items():
#     for category in categories:
#         try:
#             with open(f'{path}/model_{category.replace("/", "_")}.pkl', 'rb') as f:
#                 models[model_type][category] = pickle.load(f)
#         except FileNotFoundError:
#             logging.warning(f"Model not found for {model_type} in category {category}")

# # Prediction functions
# def predict_close_arima(category, prev_open, prev_high, prev_low):
#     model = models['ARIMA'][category]
#     exogenous_vars = [[prev_open, prev_high, prev_low]]
#     predicted_close = model.forecast(steps=1, exog=exogenous_vars).iloc[0]
#     lower_bound = predicted_close - (prev_high - prev_low)
#     upper_bound = predicted_close + (prev_high - prev_low)
#     return lower_bound, upper_bound

# def predict_close_lstm(category, prev_open, prev_high, prev_low):
#     model_info = models['LSTM'][category]
#     model = model_info['model']
#     scaler_X = model_info['scaler_X']
#     scaler_y = model_info['scaler_y']
#     input_data = np.array([[prev_open, prev_high, prev_low]])
#     input_data_scaled = scaler_X.transform(input_data)
#     input_data_lstm = input_data_scaled.reshape((1, 1, input_data_scaled.shape[1]))
#     predicted_close_scaled = model.predict(input_data_lstm)
#     predicted_close = scaler_y.inverse_transform(predicted_close_scaled)
#     output = predicted_close[0][0]
#     lower_value = output - (prev_high - prev_low)
#     upper_value = output + (prev_high - prev_low)
#     return lower_value, upper_value

# def predict_close_random_forest(category, prev_open, prev_high, prev_low):
#     model_info = models['Random Forest'][category]
#     model = model_info['model']
#     input_data = np.array([[prev_open, prev_high, prev_low]])
#     predicted_close = model.predict(input_data)[0]
#     lower_bound = predicted_close - (prev_high - prev_low)
#     upper_bound = predicted_close + (prev_high - prev_low)
#     return lower_bound, upper_bound

# def predict_close_xgboost(category, prev_open, prev_high, prev_low):
#     model_info = models['XGBoost'][category]
#     model = model_info['model']
#     input_data = np.array([[prev_open, prev_high, prev_low]])
#     dmatrix = xgb.DMatrix(input_data)
#     predicted_close = model.predict(dmatrix)[0]
#     lower_bound = predicted_close - (prev_high - prev_low)
#     upper_bound = predicted_close + (prev_high - prev_low)
#     return lower_bound, upper_bound

# @app.route('/categories', methods=['GET'])
# def get_categories():
#     logging.debug("Fetching categories")
#     return jsonify(categories)

# @app.route('/predict', methods=['POST'])
# def predict():
#     data = request.json
#     model_type = data.get('model_type')
#     category = data.get('category')
#     prev_open = data.get('prev_open')
#     prev_high = data.get('prev_high')
#     prev_low = data.get('prev_low')

#     logging.debug(f"Received prediction request for model type: {model_type}, category: {category}, prev_open: {prev_open}, prev_high: {prev_high}, prev_low: {prev_low}")

#     if category not in categories:
#         logging.error("Invalid category")
#         return jsonify({'error': 'Invalid category'}), 400

#     if not all([prev_open, prev_high, prev_low]):
#         logging.error("Missing input data")
#         return jsonify({'error': 'Missing input data'}), 400

#     if model_type == 'ARIMA':
#         lower_bound, upper_bound = predict_close_arima(category, prev_open, prev_high, prev_low)
#     elif model_type == 'LSTM':
#         lower_bound, upper_bound = predict_close_lstm(category, prev_open, prev_high, prev_low)
#     elif model_type == 'Random Forest':
#         lower_bound, upper_bound = predict_close_random_forest(category, prev_open, prev_high, prev_low)
#     elif model_type == 'XGBoost':
#         lower_bound, upper_bound = predict_close_xgboost(category, prev_open, prev_high, prev_low)
#     else:
#         logging.error("Invalid model type")
#         return jsonify({'error': 'Invalid model type'}), 400

#     logging.debug(f"Predicted bounds: lower_bound={lower_bound}, upper_bound={upper_bound}")
#     return jsonify({'lower_bound': lower_bound, 'upper_bound': upper_bound})

# if __name__ == '__main__':
#     logging.debug("Starting Flask server")
#     app.run(host='0.0.0.0', port=8500)


from flask import Flask, request, jsonify
import pandas as pd
import numpy as np
import pickle
import xgboost as xgb
from tensorflow.keras.models import load_model
import logging

# Set up logging
logging.basicConfig(level=logging.DEBUG)

app = Flask(__name__)

# Load the dataset to get category options
try:
    # data = pd.read_csv('/content/drive/MyDrive/haha/stocks_data.csv')
    categories = data['category'].unique().tolist()
except Exception as e:
    logging.error(f"Failed to load categories: {e}")
    categories = []

# Load the pre-trained models
model_paths = {
    'ARIMA': '/content/drive/MyDrive/haha/models/arima',
    'LSTM': '/content/drive/MyDrive/haha/models/lstm',
    'Random Forest': '/content/drive/MyDrive/haha/models/randomforest',
    'XGBoost': '/content/drive/MyDrive/haha/models/xgboost'
}

models = {model_type: {} for model_type in model_paths.keys()}

for model_type, path in model_paths.items():
    for category in categories:
        try:
            with open(f'{path}/model_{category.replace("/", "_")}.pkl', 'rb') as f:
                models[model_type][category] = pickle.load(f)
        except FileNotFoundError:
            logging.warning(f"Model not found for {model_type} in category {category}")

# Prediction functions
def predict_close_arima(category, prev_open, prev_high, prev_low):
    model = models['ARIMA'][category]
    exogenous_vars = [[prev_open, prev_high, prev_low]]
    predicted_close = model.forecast(steps=1, exog=exogenous_vars).iloc[0]
    lower_bound = predicted_close - (prev_high - prev_low)
    upper_bound = predicted_close + (prev_high - prev_low)
    return lower_bound, upper_bound

def predict_close_lstm(category, prev_open, prev_high, prev_low):
    model_info = models['LSTM'][category]
    model = model_info['model']
    scaler_X = model_info['scaler_X']
    scaler_y = model_info['scaler_y']
    input_data = np.array([[prev_open, prev_high, prev_low]])
    input_data_scaled = scaler_X.transform(input_data)
    input_data_lstm = input_data_scaled.reshape((1, 1, input_data_scaled.shape[1]))
    predicted_close_scaled = model.predict(input_data_lstm)
    predicted_close = scaler_y.inverse_transform(predicted_close_scaled)
    output = predicted_close[0][0]
    lower_value = output - (prev_high - prev_low)
    upper_value = output + (prev_high - prev_low)
    return lower_value, upper_value

def predict_close_random_forest(category, prev_open, prev_high, prev_low):
    model_info = models['Random Forest'][category]
    model = model_info['model']
    input_data = np.array([[prev_open, prev_high, prev_low]])
    predicted_close = model.predict(input_data)[0]
    lower_bound = predicted_close - (prev_high - prev_low)
    upper_bound = predicted_close + (prev_high - prev_low)
    return lower_bound, upper_bound

def predict_close_xgboost(category, prev_open, prev_high, prev_low):
    model_info = models['XGBoost'][category]
    model = model_info['model']
    input_data = np.array([[prev_open, prev_high, prev_low]])
    dmatrix = xgb.DMatrix(input_data)
    predicted_close = model.predict(dmatrix)[0]
    lower_bound = predicted_close - (prev_high - prev_low)
    upper_bound = predicted_close + (prev_high - prev_low)
    return lower_bound, upper_bound

@app.route('/categories', methods=['GET'])
def get_categories():
    logging.debug("Fetching categories")
    return jsonify(categories)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    model_type = data.get('model_type')
    category = data.get('category')
    prev_open = data.get('prev_open')
    prev_high = data.get('prev_high')
    prev_low = data.get('prev_low')

    logging.debug(f"Received prediction request for model type: {model_type}, category: {category}, prev_open: {prev_open}, prev_high: {prev_high}, prev_low: {prev_low}")

    if category not in categories:
        logging.error("Invalid category")
        return jsonify({'error': 'Invalid category'}), 400

    if not all([prev_open, prev_high, prev_low]):
        logging.error("Missing input data")
        return jsonify({'error': 'Missing input data'}), 400

    if model_type == 'ARIMA':
        lower_bound, upper_bound = predict_close_arima(category, prev_open, prev_high, prev_low)
    elif model_type == 'LSTM':
        lower_bound, upper_bound = predict_close_lstm(category, prev_open, prev_high, prev_low)
    elif model_type == 'Random Forest':
        lower_bound, upper_bound = predict_close_random_forest(category, prev_open, prev_high, prev_low)
    elif model_type == 'XGBoost':
        lower_bound, upper_bound = predict_close_xgboost(category, prev_open, prev_high, prev_low)
    else:
        logging.error("Invalid model type")
        return jsonify({'error': 'Invalid model type'}), 400

    logging.debug(f"Predicted bounds: lower_bound={lower_bound}, upper_bound={upper_bound}")
    return jsonify({'lower_bound': lower_bound, 'upper_bound': upper_bound})

if __name__ == '__main__':
    logging.debug("Starting Flask server")
    app.run(host='0.0.0.0', port=8500)

# curl -X POST http://your_ec2_public_dns:5000/predict -H "Content-Type: application/json" -d'{
#     "model_type": "ARIMA",
#     "category": "some_category",
#     "prev_open": 100.0,
#     "prev_high": 110.0,
#     "prev_low": 90.0,
#  }

!pip install requests

import requests

# URL of your Flask application:-
url = 'http://ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com:8500/predict'  #<- Replace with your actual public DNS or IP here








# Define headers and data payload
headers = {'Content-Type': 'application/json'}
data = {
    'model_type': 'ARIMA',
    'category': 'Banking',
    'prev_open': 100.0,
    'prev_high': 110.0,
    'prev_low': 90.0
}

# Send POST request
response = requests.post(url, headers=headers, json=data)

# Print the response
if response.status_code == 200:
    print("Response:", response.json())
else:
    print(f"Error: {response.status_code}")
    print("Response:", response.text)